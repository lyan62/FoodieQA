{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MIC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g6/g_wpvgys6_5d49rz8jrzmm840000gn/T/ipykernel_90855/1878988270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For T5 based model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstructblip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstructBlipConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstructBlipModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstructBlipPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInstructBlipForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInstructBlipProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MIC'"
     ]
    }
   ],
   "source": [
    "# For T5 based model\n",
    "from MIC.model.instructblip import InstructBlipConfig, InstructBlipModel, InstructBlipPreTrainedModel,InstructBlipForConditionalGeneration,InstructBlipProcessor\n",
    "import datasets\n",
    "import json\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch3/wenyan/cache'\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load mutli-image vqa questions\n",
    "data_dir = \"/scratch3/wenyan/data/foodie\"\n",
    "question_file = os.path.join(data_dir, \"mivqa_filtere_bi.json\")\n",
    "# mivqa = datasets.load_dataset('json', data_files=question_file)['train']\n",
    "with open(question_file, 'r') as f:\n",
    "    mivqa = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_type=\"instructblip\"\n",
    "# use large model\n",
    "# model_ckpt=\"BleachNick/MMICL-Instructblip-T5-xxl\"\n",
    "# processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n",
    "\n",
    "# use small model\n",
    "model_ckpt = \"BleachNick/MMICL-Instructblip-T5-xl\"\n",
    "processor_ckpt = \"Salesforce/instructblip-flan-t5-xl\"\n",
    "\n",
    "config = InstructBlipConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "if 'instructblip' in model_type:\n",
    "    model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "        model_ckpt,\n",
    "        config=config, cache_dir=os.environ[\"HF_HOME\"]).to(DEVICE,dtype=torch.bfloat16) \n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    processor_ckpt,cache_dir=os.environ[\"HF_HOME\"]\n",
    ")\n",
    "\n",
    "image_palceholder=\"图\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "\n",
    "sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "replace_token=\"\".join(32*[image_palceholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(question):\n",
    "    image_choices = question[\"images\"]\n",
    "    return [Image.open(os.path.join(data_dir, image_choice)) for image_choice in image_choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(question, template=0):\n",
    "    if template == 0:\n",
    "        prompt = f'<image0>{replace_token}, <image1>{replace_token}, <image2>{replace_token} <image3> {replace_token}\\n'+ 'Answer the following question according to the provided four images, they corresponds to Option (A), Option (B), Option (C), Option (D). Choose one best answer from the given options. Question: {question}, your answer is: Option ('\n",
    "    elif template == 1:\n",
    "        prompt = f'Answer the following question according to the provided four images which corresponds \n",
    "        to Option (A), Option (B), Option (C), Option (D). Choose one best answer from the given options. The options are: <image0>{replace_token} Option (A)\\n<image1>{replace_token} Option (B)\\n, <image2>{replace_token} Option (C)\\n<image3> {replace_token} Option (D)\\nQuestion: {question}, your answer is: Option ('\n",
    "    elif template == 2:\n",
    "        prompt = f'Answer the following question according to the provided four images, \n",
    "        and choose one best answer from the given options. The options are: <image0>{replace_token} Option (A)\\n<image1>{replace_token} Option (B)\\n, <image2>{replace_token} Option (C)\\n<image3> {replace_token} Option (D)\\nQuestion: {question}, your answer is: Option ('\n",
    "    elif template ==3:\n",
    "        prompt = f\"Human: Question {question} The options are: \n",
    "                    Option (A)<image0>{replace_token}\\n Option (B)<image1>{replace_token}\\n Option (C)<image2>{replace_token}\\n Option (D)<image3> {replace_token}\\nAssistant: If I have to choose one best answer from the given options， the answer is：Option (\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate 0-shot\n",
    "\n",
    "def eval_mcl(mivqa, idx, template=0):\n",
    "        question = mivqa[idx]\n",
    "        images = load_images(question)\n",
    "        prompt = get_prompt(question[\"question_bi\"], template=template)\n",
    "        inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "        inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model.generate(\n",
    "                pixel_values = inputs['pixel_values'],\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                img_mask = inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=160,\n",
    "                min_length=50,\n",
    "                num_beams=8,\n",
    "                set_min_padding_size =False,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        print(\"prompt: \", prompt)\n",
    "        print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(sample, is_shot=False):\n",
    "    question = sample[\"question\"]\n",
    "    if is_shot:\n",
    "        answer = sample[\"gt\"]\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image {answer}\\n']\n",
    "    else:\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image ']\n",
    "    return prompt\n",
    "\n",
    "def format_nshot_question(questions, cur_question, n=1):\n",
    "    final_prompt = f''\n",
    "    for i in range(n):\n",
    "        question = questions[i]\n",
    "        prompt = format_question(question, is_shot=True)\n",
    "        final_prompt += prompt[0]\n",
    "    \n",
    "    final_prompt += format_question(cur_question)[0]\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "## evaluate\n",
    "\n",
    "images = load_images(mivqa[n_shot])\n",
    "inputs = processor(images=images, text=prompt[0], return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=160,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(\"prompt: \", prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 哪一道菜中含有土豆？ Answer: image 3\\nUse the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 下面哪道菜有家禽肉? Answer: image ']\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  ['Use the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 哪一道菜中含有土豆？ Answer: image 3\\nUse the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 下面哪道菜有家禽肉? Answer: image ']\n",
      "chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chines\n"
     ]
    }
   ],
   "source": [
    "def format_question(sample, is_shot=False):\n",
    "    question = sample[\"question\"]\n",
    "    if is_shot:\n",
    "        answer = sample[\"gt\"]\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image {answer}\\n']\n",
    "    else:\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image ']\n",
    "    return prompt\n",
    "\n",
    "def format_nshot_question(questions, cur_question, n=1):\n",
    "    final_prompt = f''\n",
    "    for i in range(n):\n",
    "        question = questions[i]\n",
    "        prompt = format_question(question, is_shot=True)\n",
    "        final_prompt += prompt[0]\n",
    "    \n",
    "    final_prompt += format_question(cur_question)[0]\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "## 1 shot\n",
    "n_shot = 1\n",
    "n_shot_images = []\n",
    "for i in range(n_shot):\n",
    "    n_shot_images.extend(load_images(mivqa[i]))\n",
    "\n",
    "prompt = format_nshot_question(mivqa, mivqa[n_shot], n_shot)\n",
    "question_images = load_images(mivqa[n_shot])\n",
    "print(prompt)\n",
    "\n",
    "# images = load_images(mivqa[0])\n",
    "# prompt = format_question(mivqa[0][\"question\"])\n",
    "\n",
    "n_shot_images.extend(question_images)\n",
    "print(len(n_shot_images))\n",
    "# max_length = max([len(f) for f in images ])\n",
    "# print(max_length)\n",
    "images = n_shot_images\n",
    "inputs = processor(images=images, text=prompt[0], return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=160,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(\"prompt: \", prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open (\"images/flamingo_photo.png\")\n",
    "image1 = Image.open (\"images/flamingo_cartoon.png\")\n",
    "image2 = Image.open (\"images/flamingo_3d.png\")\n",
    "\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aids to help you answer the question. Question: Give the reason why image 0, image 1 and image 2 are different? Answer:']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=80,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519532/273255245.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a flamingo.', 'flamingo standing in the water.']\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import pad\n",
    "def padd_images( image, max_length):\n",
    "    image = torch.tensor(image)\n",
    "    mask = torch.zeros(max_length).bool()\n",
    "    pad_len = max_length - image.shape[0]\n",
    "    mask[:image.shape[0]] = True\n",
    "    image = pad(image,(0,0,0,0,0,0,0,pad_len)) # padding behind the first dim\n",
    "    return image,mask\n",
    "\n",
    "image = Image.open (\"images/chinchilla.png\")\n",
    "image1 = Image.open (\"images/shiba.png\")\n",
    "image2 = Image.open (\"images/flamingo.png\")\n",
    "\n",
    "image4 = Image.open (\"images/shiba.png\")\n",
    "image5 = Image.open (\"images/flamingo.png\")\n",
    "\n",
    "images =[ \n",
    "[image,image1,image2], [image4 ,image5]\n",
    "]\n",
    "\n",
    "prompt = [f'image 0 is <image0>{replace_token},image 1 is <image1>{replace_token},image 2 is <image2>{replace_token}. Question: <image0> is a chinchilla. They are mainly found in Chile.\\n Question: <image1> is a shiba. They are very popular in Japan.\\nQuestion: image 2 is',\n",
    "f'image 0 is <image0>{replace_token}, image 0 is a shiba. They are very popular in Japan.\\n image 1 is <image1>{replace_token}, image 1 is a',\n",
    "]\n",
    "\n",
    "max_image_length = max([len(f) for f in images ])\n",
    "\n",
    "\n",
    "inputs = processor( text=prompt, return_tensors=\"pt\",padding=True) \n",
    "\n",
    "pixel_values= [ processor(images=img, return_tensors=\"pt\")['pixel_values'] for img in images]\n",
    "\n",
    "image_list=[]\n",
    "mask_list= []\n",
    "for img in pixel_values:\n",
    "    image,img_mask = padd_images(img,max_image_length)\n",
    "    image_list.append(image)\n",
    "    mask_list.append(img_mask)\n",
    "inputs['pixel_values'] = torch.stack(image_list).to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.stack(mask_list)\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x6\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/cal_num1.png\")\n",
    "image1 = Image.open (\"images/cal_num2.png\")\n",
    "image2 = Image.open (\"images/cal_num3.png\")\n",
    "images = [image,image1,image2]\n",
    "\n",
    "prompt = [f'Use the image 0: <image0>{replace_token},image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aid to help you calculate the equation accurately. image 0 is 2+1=3.\\nimage 1 is 5+6=11.\\nimage 2 is\"']\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a flamingo. They are native to South America and are known for their bright red plumage and distinctive call.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image = Image.open (\"images/chinchilla.png\")\n",
    "image1 = Image.open (\"images/shiba.png\")\n",
    "image2 = Image.open (\"images/flamingo.png\")\n",
    "images = [image,image1,image2]\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'image 0 is <image0>{replace_token},image 1 is <image1>{replace_token},image 2 is <image2>{replace_token}. Question: <image0> is a chinchilla. They are mainly found in Chile.\\n Question: <image1> is a shiba. They are very popular in Japan.\\nQuestion: image 2 is']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0 is a flamingo and image 1 is a cartoon of a flamingo and image 2 is a cartoon of a flamingo a flamingo a flamingo a flamingo a flamingo a flamingo a flamingo a flaming\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/flamingo_photo.png\")\n",
    "image1 = Image.open (\"images/flamingo_cartoon.png\")\n",
    "image2 = Image.open (\"images/flamingo_3d.png\")\n",
    "\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aids to help you answer the question. Question: Give the reason why image 0, image 1 and image 2 are different? Answer:']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=80,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
