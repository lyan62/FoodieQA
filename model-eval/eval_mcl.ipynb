{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T5 based model\n",
    "from MIC.model.instructblip import InstructBlipConfig, InstructBlipModel, InstructBlipPreTrainedModel,InstructBlipForConditionalGeneration,InstructBlipProcessor\n",
    "import datasets\n",
    "import json\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch3/wenyan/cache'\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load mutli-image vqa questions\n",
    "data_dir = \"/scratch3/wenyan/data/foodie\"\n",
    "question_file = os.path.join(data_dir, \"multi-image-vqa-ann.json\")\n",
    "# mivqa = datasets.load_dataset('json', data_files=question_file)['train']\n",
    "with open(question_file, 'r') as f:\n",
    "    mivqa = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '哪一道菜中含有土豆？',\n",
       " 'choices': '',\n",
       " 'user-answer': '3',\n",
       " 'gt': '3',\n",
       " 'rationale': 'D中含有土豆，其他均没有',\n",
       " 'bad_q': 'No',\n",
       " 'num_hops': 'single',\n",
       " 'question_id': '316ee2c7ac04c1a3e636175a8d42c6ec_2',\n",
       " 'session_id': 'val-a6b03c47b0cb48b1a7c03504b4a7bf4c',\n",
       " 'ann_group': '湘',\n",
       " 'images': ['14456664_all_202404292352223293/104_IMG_0277.jpeg',\n",
       "  '14456664_all_202404292352223293/132_IMG_20220702_130156.jpg',\n",
       "  '14456664_all_202404292352223293/147_IMG_20190225_184723.jpg',\n",
       "  '14456664_all_202404292352223293/151_IMG_20240414_200337.jpg']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_type=\"instructblip\"\n",
    "# use large model\n",
    "# model_ckpt=\"BleachNick/MMICL-Instructblip-T5-xxl\"\n",
    "# processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n",
    "\n",
    "# use small model\n",
    "model_ckpt = \"BleachNick/MMICL-Instructblip-T5-xl\"\n",
    "processor_ckpt = \"Salesforce/instructblip-flan-t5-xl\"\n",
    "\n",
    "config = InstructBlipConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "if 'instructblip' in model_type:\n",
    "    model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "        model_ckpt,\n",
    "        config=config, cache_dir=os.environ[\"HF_HOME\"]).to(DEVICE,dtype=torch.bfloat16) \n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    processor_ckpt,cache_dir=os.environ[\"HF_HOME\"]\n",
    ")\n",
    "\n",
    "image_palceholder=\"图\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "\n",
    "sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "replace_token=\"\".join(32*[image_palceholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(question):\n",
    "    image_choices = question[\"images\"]\n",
    "    return [Image.open(os.path.join(data_dir, image_choice)) for image_choice in image_choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 哪一道菜中含有土豆？ Answer: image 3\\nUse the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 下面哪道菜有家禽肉? Answer: image ']\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  ['Use the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 哪一道菜中含有土豆？ Answer: image 3\\nUse the image 0: <image0>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 1: <image1>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, image 2: <image2>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图, and image 3: <image3>图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图图 as visual choices to answer the question. Question: 下面哪道菜有家禽肉? Answer: image ']\n",
      "chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chinese chines\n"
     ]
    }
   ],
   "source": [
    "def format_question(sample, is_shot=False):\n",
    "    question = sample[\"question\"]\n",
    "    if is_shot:\n",
    "        answer = sample[\"gt\"]\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image {answer}\\n']\n",
    "    else:\n",
    "        prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token}, image 2: <image2>{replace_token}, and image 3: <image3>{replace_token} as visual choices to answer the question. Question: {question} Answer: image ']\n",
    "    return prompt\n",
    "\n",
    "def format_nshot_question(questions, cur_question, n=1):\n",
    "    final_prompt = f''\n",
    "    for i in range(n):\n",
    "        question = questions[i]\n",
    "        prompt = format_question(question, is_shot=True)\n",
    "        final_prompt += prompt[0]\n",
    "    \n",
    "    final_prompt += format_question(cur_question)[0]\n",
    "    return [final_prompt]\n",
    "\n",
    "\n",
    "## 1 shot\n",
    "n_shot = 1\n",
    "n_shot_images = []\n",
    "for i in range(n_shot):\n",
    "    n_shot_images.extend(load_images(mivqa[i]))\n",
    "\n",
    "prompt = format_nshot_question(mivqa, mivqa[n_shot], n_shot)\n",
    "question_images = load_images(mivqa[n_shot])\n",
    "print(prompt)\n",
    "\n",
    "# images = load_images(mivqa[0])\n",
    "# prompt = format_question(mivqa[0][\"question\"])\n",
    "\n",
    "n_shot_images.extend(question_images)\n",
    "print(len(n_shot_images))\n",
    "# max_length = max([len(f) for f in images ])\n",
    "# print(max_length)\n",
    "images = n_shot_images\n",
    "inputs = processor(images=images, text=prompt[0], return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=160,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(\"prompt: \", prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open (\"images/flamingo_photo.png\")\n",
    "image1 = Image.open (\"images/flamingo_cartoon.png\")\n",
    "image2 = Image.open (\"images/flamingo_3d.png\")\n",
    "\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aids to help you answer the question. Question: Give the reason why image 0, image 1 and image 2 are different? Answer:']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=80,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519532/273255245.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a flamingo.', 'flamingo standing in the water.']\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import pad\n",
    "def padd_images( image, max_length):\n",
    "    image = torch.tensor(image)\n",
    "    mask = torch.zeros(max_length).bool()\n",
    "    pad_len = max_length - image.shape[0]\n",
    "    mask[:image.shape[0]] = True\n",
    "    image = pad(image,(0,0,0,0,0,0,0,pad_len)) # padding behind the first dim\n",
    "    return image,mask\n",
    "\n",
    "image = Image.open (\"images/chinchilla.png\")\n",
    "image1 = Image.open (\"images/shiba.png\")\n",
    "image2 = Image.open (\"images/flamingo.png\")\n",
    "\n",
    "image4 = Image.open (\"images/shiba.png\")\n",
    "image5 = Image.open (\"images/flamingo.png\")\n",
    "\n",
    "images =[ \n",
    "[image,image1,image2], [image4 ,image5]\n",
    "]\n",
    "\n",
    "prompt = [f'image 0 is <image0>{replace_token},image 1 is <image1>{replace_token},image 2 is <image2>{replace_token}. Question: <image0> is a chinchilla. They are mainly found in Chile.\\n Question: <image1> is a shiba. They are very popular in Japan.\\nQuestion: image 2 is',\n",
    "f'image 0 is <image0>{replace_token}, image 0 is a shiba. They are very popular in Japan.\\n image 1 is <image1>{replace_token}, image 1 is a',\n",
    "]\n",
    "\n",
    "max_image_length = max([len(f) for f in images ])\n",
    "\n",
    "\n",
    "inputs = processor( text=prompt, return_tensors=\"pt\",padding=True) \n",
    "\n",
    "pixel_values= [ processor(images=img, return_tensors=\"pt\")['pixel_values'] for img in images]\n",
    "\n",
    "image_list=[]\n",
    "mask_list= []\n",
    "for img in pixel_values:\n",
    "    image,img_mask = padd_images(img,max_image_length)\n",
    "    image_list.append(image)\n",
    "    mask_list.append(img_mask)\n",
    "inputs['pixel_values'] = torch.stack(image_list).to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.stack(mask_list)\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x6\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/cal_num1.png\")\n",
    "image1 = Image.open (\"images/cal_num2.png\")\n",
    "image2 = Image.open (\"images/cal_num3.png\")\n",
    "images = [image,image1,image2]\n",
    "\n",
    "prompt = [f'Use the image 0: <image0>{replace_token},image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aid to help you calculate the equation accurately. image 0 is 2+1=3.\\nimage 1 is 5+6=11.\\nimage 2 is\"']\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a flamingo. They are native to South America and are known for their bright red plumage and distinctive call.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image = Image.open (\"images/chinchilla.png\")\n",
    "image1 = Image.open (\"images/shiba.png\")\n",
    "image2 = Image.open (\"images/flamingo.png\")\n",
    "images = [image,image1,image2]\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'image 0 is <image0>{replace_token},image 1 is <image1>{replace_token},image 2 is <image2>{replace_token}. Question: <image0> is a chinchilla. They are mainly found in Chile.\\n Question: <image1> is a shiba. They are very popular in Japan.\\nQuestion: image 2 is']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to('cuda:0')\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0 is a flamingo and image 1 is a cartoon of a flamingo and image 2 is a cartoon of a flamingo a flamingo a flamingo a flamingo a flamingo a flamingo a flamingo a flaming\n"
     ]
    }
   ],
   "source": [
    "image = Image.open (\"images/flamingo_photo.png\")\n",
    "image1 = Image.open (\"images/flamingo_cartoon.png\")\n",
    "image2 = Image.open (\"images/flamingo_3d.png\")\n",
    "\n",
    "images = [image,image1,image2]\n",
    "prompt = [f'Use the image 0: <image0>{replace_token}, image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aids to help you answer the question. Question: Give the reason why image 0, image 1 and image 2 are different? Answer:']\n",
    "\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=80,\n",
    "        min_length=50,\n",
    "        num_beams=8,\n",
    "        set_min_padding_size =False,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
