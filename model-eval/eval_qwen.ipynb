{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 12:36:40,908 - modelscope - WARNING - Model revision not specified, use revision: v1.0.3\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21272fe156484f6f881e2608a1a74f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## download model\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os \n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch3/wenyan/cache'\n",
    "# DEVICE = \"cuda:1\"\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "model_dir = snapshot_download('qwen/Qwen-VL', cache_dir=os.environ['HF_HOME'])\n",
    "# model_dir = snapshot_download('qwen/Qwen-VL-Chat')\n",
    "\n",
    "\n",
    "# Loading local checkpoints\n",
    "# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, do_image_splitting=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load mutli-image vqa questions\n",
    "import json\n",
    "data_dir = \"/scratch3/wenyan/data/foodie\"\n",
    "question_file = os.path.join(data_dir, \"mivqa_filtered.json\")\n",
    "# mivqa = datasets.load_dataset('json', data_files=question_file)['train']\n",
    "with open(question_file, 'r') as f:\n",
    "    mivqa = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '哪一道菜中含有干贝？',\n",
       " 'choices': '',\n",
       " 'answer': '1',\n",
       " 'question_type': 'ingredients',\n",
       " 'question_id': '5cff42e986afc707c83ee411ae4af2e6_0',\n",
       " 'ann_group': '闽',\n",
       " 'images': ['14521898_all_202405061124164430/179_image.jpg',\n",
       "  '14521898_all_202405061124164430/208_IMG_5468.jpeg',\n",
       "  '14456664_all_202404292352223293/179_IMG_4221.jpeg',\n",
       "  '14456664_all_202404292352223293/188_57291912-AA46-487E-8EA0-01538BDAD35E.jpeg'],\n",
       " 'qid': 'mivqa-0'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = mivqa[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_list(question, data_dir, template=0):\n",
    "    q = question[\"question\"].strip()\n",
    "    if template == 0:\n",
    "        q = q.replace(\"以下\", \"以上\")\n",
    "        query_list = [{\"image\": os.path.join(data_dir, image)} for image in question[\"images\"]]\n",
    "        query_list.append({\"text\": \"根据以上四张图回答问题，他们分别为图A, 图B, 图C, 图D, 问题：{}, 答案为：图\".format(q)})\n",
    "    if template == 1:\n",
    "        query_list = []\n",
    "        images = question[\"images\"]\n",
    "        idx2choice = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\"}\n",
    "        for i in range(len(images)):\n",
    "            query_list.append({\"image\" : os.path.join(data_dir, images[i])})\n",
    "            query_list.append({\"text\" : \"图{}\\n\".format(idx2choice[i])})\n",
    "        query_list.append({\"text\": \"根据以上四张图回答问题, 问题：{}, 答案为：图\".format(q)})\n",
    "    if template == 2:\n",
    "        query_list = [{\"text\":\"根据以下四张图回答问题,\"}]\n",
    "        images = question[\"images\"]\n",
    "        idx2choice = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\"}\n",
    "        for i in range(len(images)):\n",
    "            query_list.append({\"text\" : \"图{}\".format(idx2choice[i])})\n",
    "            query_list.append({\"image\" : os.path.join(data_dir, images[i])})\n",
    "        query_list.append({\"text\": \"问题：{}， 答案为：图\".format(question[\"question\"])})\n",
    "    if template == 3:\n",
    "        query_list = [{\"image\": os.path.join(data_dir, image)} for image in question[\"images\"]]\n",
    "        query_list.append({\"text\": \"根据以上四张图回答问题, 问题：{}, 答案为：Picture\".format(question[\"question\"])})\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': '/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/104_IMG_0277.jpeg'},\n",
       " {'image': '/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/132_IMG_20220702_130156.jpg'},\n",
       " {'image': '/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/147_IMG_20190225_184723.jpg'},\n",
       " {'image': '/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/151_IMG_20240414_200337.jpg'},\n",
       " {'text': '根据以上四张图回答问题，他们分别为图A, 图B, 图C, 图D, 问题：哪一道菜中含有土豆？, 答案为：图'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list1 = get_query_list(question, data_dir)\n",
    "query_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list2 = get_query_list(question, data_dir, template=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list3 = get_query_list(question, data_dir, template=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (ln_pre): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)\n",
       "      (transformer): TransformerBlock(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-47): 48 x VisualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)\n",
       "            (ln_2): LayerNorm((1664,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): VisualAttention(\n",
       "              (in_proj): Linear(in_features=1664, out_features=4992, bias=True)\n",
       "              (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (attn_pool): Resampler(\n",
       "        (kv_proj): Linear(in_features=1664, out_features=4096, bias=False)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (ln_q): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "        (ln_kv): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (ln_post): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picture 1:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/104_IMG_0277.jpeg</img>\n",
      "Picture 2:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/132_IMG_20220702_130156.jpg</img>\n",
      "Picture 3:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/147_IMG_20190225_184723.jpg</img>\n",
      "Picture 4:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/151_IMG_20240414_200337.jpg</img>\n",
      "根据以上四张图回答问题，他们分别为图A, 图B, 图C, 图D, 问题：哪一道菜中含有土豆？, 答案为：图C<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## example query input\n",
    "query = tokenizer.from_list_format(query_list1)\n",
    "inputs = tokenizer(query, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs)\n",
    "response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Picture 1:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/104_IMG_0277.jpeg</img>\\nPicture 2:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/132_IMG_20220702_130156.jpg</img>\\nPicture 3:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/147_IMG_20190225_184723.jpg</img>\\nPicture 4:<img>/scratch3/wenyan/data/foodie/14456664_all_202404292352223293/151_IMG_20240414_200337.jpg</img>\\n根据以上四张图回答问题，他们分别为图A, 图B, 图C, 图D, 问题：哪一道菜的主料明显与别的菜不同？, 答案为：图'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def eval_qwen(mivqa, i, template=0):\n",
    "    question = mivqa[i]\n",
    "    query_list = get_query_list(question, data_dir, template=template)\n",
    "    query = tokenizer.from_list_format(query_list)\n",
    "    inputs = tokenizer(query, return_tensors='pt')\n",
    "    inputs = inputs.to(model.device)\n",
    "    pred = model.generate(**inputs)\n",
    "    response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"qid\": mivqa[i][\"qid\"]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [09:42<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/scratch3/wenyan/data/foodie/mivqa_qwen_temp0.jsonl\", \"w\") as f:\n",
    "    for i in tqdm(range(len(mivqa))):\n",
    "        res = eval_qwen(mivqa, i, template=0)\n",
    "        f.write(json.dumps(res, ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [09:18<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/scratch3/wenyan/data/foodie/mivqa_qwen_temp1.jsonl\", \"w\") as f:\n",
    "    for i in tqdm(range(len(mivqa))):\n",
    "        res = eval_qwen(mivqa, i, template=1)\n",
    "        f.write(json.dumps(res, ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
