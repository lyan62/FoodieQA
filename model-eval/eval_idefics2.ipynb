{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch3/wenyan/cache'\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n",
    "image1 = load_image(\"/scratch3/wenyan/code/foodie-eval/model-eval/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "image2 = load_image(\"/scratch3/wenyan/code/foodie-eval/model-eval/Skyline-Chicago.jpg\")\n",
    "image3 = load_image(\"/scratch3/wenyan/code/foodie-eval/model-eval/Golden-Gate-Bridge-San-Francisco.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a3ff90132a4dab91f5e54bddb5a454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", \n",
    "                                          cache_dir=os.environ[\"HF_HOME\"], do_image_splitting=False)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\", cache_dir=os.environ[\"HF_HOME\"], device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User: What do we see in this image? \\nAssistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. \\nUser: 这张照片呢其天气如何请用中文回答? \\nAssistant: 天空很暗。.']\n"
     ]
    }
   ],
   "source": [
    "# Create inputs\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"这张照片呢其天气如何请用中文回答?\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
    "inputs = {k: v.to() for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read mivqa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = \"/scratch3/wenyan/data/foodie\"\n",
    "mivqa_file = \"mivqa_filtered.json\"\n",
    "\n",
    "with open(os.path.join(data_dir, mivqa_file), \"r\") as f:\n",
    "    mivqa = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image_input(img_idx, template=0):\n",
    "    idx2choice = {\n",
    "        0: \"A\",\n",
    "        1: \"B\",\n",
    "        2: \"C\",\n",
    "        3: \"D\"\n",
    "    }\n",
    "    if template == 0:\n",
    "        img_input = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                    ]\n",
    "        }\n",
    "    if template == 1:\n",
    "        img_input = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": \"图\"+idx2choice[img_idx]},\n",
    "                    ]\n",
    "        }\n",
    "    return img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_input(question, template=0):\n",
    "    q = question[\"question\"]\n",
    "    if template == 0:\n",
    "        if \"以下\" in q:\n",
    "            q=q.replace(\"以下\", \"以上\")\n",
    "        text_input = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"根据以上四张图回答问题，他们分别为图A, 图B, 图C, 图D, 问题：{}, 答案为：图\".format(q)},\n",
    "                    ]\n",
    "        }\n",
    "    if template == 1:\n",
    "        if \"以下\" in q:\n",
    "            q=q.replace(\"以下\", \"以上\")\n",
    "        text_input = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"根据以上四张图回答问题, 问题：{}, 答案为：图\".format(q)},\n",
    "                    ]\n",
    "        }\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(mivqa, idx, img_template=1, text_template=0):\n",
    "    messages = []\n",
    "    question = mivqa[idx]\n",
    "    for i in range(4):\n",
    "        img_input = format_image_input(i, template=img_template)\n",
    "        messages.append(img_input)\n",
    "    text_input = format_text_input(question, template=text_template)\n",
    "    messages.append(text_input)\n",
    "    \n",
    "    images = [load_image(os.path.join(data_dir, img)) for img in question[\"images\"]]\n",
    "    return messages, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'text', 'text': '图A'}]}, {'role': 'user', 'content': [{'type': 'image'}, {'type': 'text', 'text': '图B'}]}, {'role': 'user', 'content': [{'type': 'image'}, {'type': 'text', 'text': '图C'}]}, {'role': 'user', 'content': [{'type': 'image'}, {'type': 'text', 'text': '图D'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '根据以上四张图回答问题, 问题：哪一道菜的色泽最鲜艳？, 答案为：图'}]}]\n"
     ]
    }
   ],
   "source": [
    "messages, images = build_input(mivqa, 4, img_template=1, text_template=1)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User: 图A \\nUser: 图B \\nUser: 图C \\nUser: 图D \\nUser: 根据以上四张图回答问题, 问题：哪一道菜的色泽最鲜艳？, 答案为：图 \\nAssistant: 图A.']\n"
     ]
    }
   ],
   "source": [
    "# Create inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "inputs = {k: v.to() for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User: 图A \\nUser: 图B \\nUser: 图C \\nUser: 图D \\nUser: 根据以上四张图回答问题, 问题：哪一道菜的色泽最鲜艳？, 答案为：图 ',\n",
       " '图A.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[0].split(\"\\nAssistant: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_question(mivqa, idx, img_template=0, text_template=0):\n",
    "    messages, images = build_input(mivqa, idx, img_template=img_template, text_template=text_template)\n",
    "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to() for k, v in inputs.items()}\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return {\n",
    "        \"response\": generated_texts,\n",
    "        \"qid\": mivqa[idx][\"qid\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [25:14<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(\"/scratch3/wenyan/data/foodie/mivqa_idefics2_temp0.jsonl\", \"w\") as f:\n",
    "    for i in tqdm(range(len(mivqa))):\n",
    "        res = eval_question(mivqa, i, img_template=0, text_template=0)\n",
    "        f.write(json.dumps(res)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/397 [00:00<?, ?it/s]/scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 397/397 [27:07<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/scratch3/wenyan/data/foodie/mivqa_idefics2_temp1.jsonl\", \"w\") as f:\n",
    "    for i in tqdm(range(len(mivqa))):\n",
    "        res = eval_question(mivqa, i, img_template=1, text_template=1)\n",
    "        f.write(json.dumps(res, ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /scratch3/wenyan/miniconda3/envs/dl/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def parse_output(res):\n",
    "    ans = res[\"response\"][0].split(\"\\nAssistant: \")[1].split(\"图\")[1][0]\n",
    "    ans2idx = {\n",
    "        \"A\":\"0\",\n",
    "        \"B\":\"1\",\n",
    "        \"C\":\"2\",\n",
    "        \"D\":\"3\"\n",
    "    }\n",
    "    return ans2idx[ans.upper()]\n",
    "\n",
    "def get_accuracy(result_file, mivqa):\n",
    "    # get gts\n",
    "    gt = [x[\"answer\"] for x in mivqa]\n",
    "    \n",
    "    # get all answers\n",
    "    data = []\n",
    "    with open(result_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    ## get answers\n",
    "    all_answers = []\n",
    "    for d in data:\n",
    "        try:\n",
    "            ans = parse_output(d)\n",
    "            all_answers.append(ans)\n",
    "        except:\n",
    "            print(d[\"qid\"], d)\n",
    "    \n",
    "    accuracy = accuracy_score(all_answers, gt)\n",
    "    print(\"accuracy is: \", accuracy)\n",
    "    return accuracy\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.36523929471032746\n"
     ]
    }
   ],
   "source": [
    "acc_prompt1 = get_accuracy(\"/scratch3/wenyan/data/foodie/mivqa_idefics2_temp0.jsonl\", mivqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.4836272040302267\n"
     ]
    }
   ],
   "source": [
    "acc_prompt2 = get_accuracy(\"/scratch3/wenyan/data/foodie/mivqa_idefics2_temp1.jsonl\", mivqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
